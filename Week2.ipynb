{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d136cd4",
   "metadata": {},
   "source": [
    "# Intro to Logistic Regression in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa16a08",
   "metadata": {},
   "source": [
    "In this notebook, we're going to build a very simple neural network in PyTorch to do handwritten digit classification.\n",
    "First, we'll start with some exploration of the MNIST dataset, explaining how we load and format the data.\n",
    "We'll then jump into motivating and then implementing the logistic regression model, including the forward and backwards pass, loss functions, and optimizers.\n",
    "After training the model, we'll evaluate how we did and visualize what we've learned.\n",
    "Finally, we'll refactor our code in an object-oriented manner, using higher level APIs.\n",
    "\n",
    "Before we get started, some imports for the packages we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849f0b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c6583",
   "metadata": {},
   "source": [
    "### MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e03be",
   "metadata": {},
   "source": [
    "The MNIST dataset is very popular machine learning dataset, consisting of 70000 grayscale images of handwritten digits, of dimensions 28x28. \n",
    "We'll be using it as our example dataset for this section of the tutorial, with the goal being to predict which digit is in each image.\n",
    "\n",
    "![mnist](Figures/mnist.png)\n",
    "\n",
    "The first (and often most important) step in machine learning is preparing the data.\n",
    "This can include downloading, organizing, formatting, shuffling, pre-processing, augmenting, and batching examples so that they can be fed to a model.\n",
    "The `torchvision` package makes this easy by implementing many of these, allowing us to put these datasets into a usable form in only a few lines of code.\n",
    "First, let's download the train and test sets of MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1535cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "mnist_train = datasets.MNIST(root=\"./datasets\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = datasets.MNIST(root=\"./datasets\", train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9b6146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MNIST training examples: 60000\n",
      "Number of MNIST test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of MNIST training examples: {}\".format(len(mnist_train)))\n",
    "print(\"Number of MNIST test examples: {}\".format(len(mnist_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f27fae",
   "metadata": {},
   "source": [
    "As we'd expect, 60000 of the MNIST examples are in the train set, and the rest are in the test set.\n",
    "We added the transform `ToTensor()` when formatting the dataset, to convert the input data from a Pillow `Image` type into a PyTorch `Tensor`. Tensors will eventually be the input type that we feed into our model. \n",
    "\n",
    "Let's look at an example image from the train set and its label.\n",
    "Notice that the `image` tensor defaults to something 3-dimensional.\n",
    "The \"1\" in the first dimension indicates that the image only has one channel (i.e. grayscale).\n",
    "We need to get rid of this to visualize the image with `imshow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "331b791e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default image shape: torch.Size([1, 28, 28])\n",
      "Reshaped image shape: torch.Size([28, 28])\n",
      "The label for this image: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pick out the 4th (0-indexed) example from the training set\n",
    "image, label = mnist_train[3]\n",
    "\n",
    "# Plot the image\n",
    "print(\"Default image shape: {}\".format(image.shape))\n",
    "image = image.reshape([28,28])\n",
    "print(\"Reshaped image shape: {}\".format(image.shape))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "# Print the label\n",
    "print(\"The label for this image: {}\".format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7afa4",
   "metadata": {},
   "source": [
    "While we could work directly with the data as a `torchvision.dataset`, we'll find it useful to use a `DataLoader`, which will take care of shuffling and batching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be62710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf7abc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the minibatch of images: torch.Size([100, 1, 28, 28])\n",
      "Shape of the minibatch of labels: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "data_train_iter = iter(train_loader)\n",
    "images, labels = data_train_iter.next()\n",
    "\n",
    "print(\"Shape of the minibatch of images: {}\".format(images.shape))\n",
    "print(\"Shape of the minibatch of labels: {}\".format(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4696cfbd",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835354ca",
   "metadata": {},
   "source": [
    "Now that we have a good feel for how to load our data, let's start putting together our model. \n",
    "In this tutorial, we'll be building a logistic regression model, which is essentially a fully-connected neural network without any hidden layers. \n",
    "While fairly basic, logistic regression can perform surprisingly well on many simple classification tasks.\n",
    "\n",
    "#### The forward pass\n",
    "\n",
    "While our data inputs (which we'll call `x`) are images (i.e. 2-dimensional), MNIST digits are pretty small, and the model we're using is very simple.\n",
    "Thus, we're going to be treating the input as flat vectors.\n",
    "To convert our inputs into row vectors (a.k.a. flattening), we can use `view()`, the equivalent of NumPy's `reshape()`.\n",
    "Also like NumPy, we can replace one of the dimensions of the reshaping with a `-1`, which tells PyTorch to infer this dimension based on the original dimensions and the other specified dimensions.\n",
    "Let's do try this flattening on the minibatch of 100 images we drew in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f5bfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of input x: torch.Size([100, 784])\n"
     ]
    }
   ],
   "source": [
    "x = images.view(-1, 28*28)\n",
    "print(\"The shape of input x: {}\".format(x.shape))# cambio las matrices de 28x28 por vectores\n",
    "# de 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8fd480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "651249c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights W\n",
    "W = torch.randn(784, 10)/np.sqrt(784)\n",
    "W.requires_grad_()\n",
    "\n",
    "# Initialize bias b as 0s\n",
    "b = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd643669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb9b3a",
   "metadata": {},
   "source": [
    "As both `W` and `b` are parameters we wish to learn, we set `requires_grad` to `True`. \n",
    "This tells PyTorch's autograd to track the gradients for these two variables, and all the variables depending on `W` and `b`.\n",
    "\n",
    "With these model parameters, we compute $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21e36eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear transformation with W and b\n",
    "y = torch.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62f48de",
   "metadata": {},
   "source": [
    "We can see for example what the predictions look like for the first example in our minibatch. Remember, the bigger the number, the more the model thinks the input $x$ is of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bb1311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3128,  0.1433,  0.0745, -0.0265, -0.0025, -0.0903, -0.2322, -0.6388,\n",
      "         0.3499,  0.2201], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(y[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4bffa",
   "metadata": {},
   "source": [
    "Convert values to probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1424787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py[0] from equation: tensor([0.1306, 0.1102, 0.1029, 0.0930, 0.0953, 0.0873, 0.0757, 0.0504, 0.1355,\n",
      "        0.1190], grad_fn=<SelectBackward>)\n",
      "py[0] with torch.nn.functional.softmax: tensor([0.1306, 0.1102, 0.1029, 0.0930, 0.0953, 0.0873, 0.0757, 0.0504, 0.1355,\n",
      "        0.1190], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Softmax to probabilities from equation\n",
    "py_eq = torch.exp(y) / torch.sum(torch.exp(y), dim=1, keepdim=True)\n",
    "print(\"py[0] from equation: {}\".format(py_eq[0]))\n",
    "\n",
    "# Option 2: Softmax to probabilities with torch.nn.functional\n",
    "import torch.nn.functional as F\n",
    "py = F.softmax(y, dim=1)\n",
    "print(\"py[0] with torch.nn.functional.softmax: {}\".format(py[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49cce5",
   "metadata": {},
   "source": [
    "We've now defined the forward pass of our model: given an input image, the graph returns the probabilities the model thinks the input is each of the 10 classes. Are we done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a57638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
